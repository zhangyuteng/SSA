SSA算法实现[Semantic Relatedness Using Salient Semantic Analysis]
第一步:规范维基百科的Dump,保留正文和链接信息,并将链接用[[ ]]标注.
    命令1:python WikiExtractor.py ../Corpus/enwiki-20161001-pages-articles-multistream.xml --output ../WikiOutput/OutputofWikiExtractor20170330.ver/ --lustyle --html --bytes 200M --filter_disambig_pages --min_text_length 100
        目的是，只保留纯粹的Article页面。消歧面要过滤掉，同时non stopwords的数量少于100的，也要过滤掉。此外，如ESA(by Cagatay Calli)中提到的那些、标了红色的也去掉(见2017-02-11日志)。
    命令2:python ParseWikiExtractorOut.py ../WikiOutput/OutputofWikiExtractor20170330.ver/AA/ --output ../WikiOutputParse/WE0330_PWEO0330/
        这个程序是对OutputofWikiExtractor20170330.ver/AA/目录下的文件进行词性标注、词形还原处理，生成的结果文件就保存在cluster的../WikiOutputParser/ WE0330_PWEO0330目录下。
第二步:利用one sense per discourse假设添加链接信息.
    命令3:Python 2step.py ../WikiOutputParse/WE0330_PWEO0330/ -o ../ReduceWikiOutput
第三步:统计人工添加的链接信息和机器添加(第二步的操作)的链接信息.


Python Version == 2.7









